---
title: Word2Vec 基础
date: 2018-06-17 23:28:38
tags:
- NLP
- Word2Vec
categories: 笔记
---

## NLP中常见的任务
1. 自动摘要
2. 指代消解
3. 机器翻译
4. 词性标注
5. 分词（中文、日文等）
6. 主题识别
7. 文本分类

由于计算机无法理解原始的自然语言，所以我们需要对自然语言进行编码，编码的结果需要保证词的相似性，向量空间分布的相似性。最终达到的目的是词向量表示作为机器学习、特别是深度学习的输入和表示空间。

## one-hot编码
最初使用的是one-hot编码，需要统计字典然后对每个词进行one-hot编码。所存在的缺陷是：1)无法衡量向量间的关系；2)数据稀疏；3)如果有新词需要不断更新字典重新编码
而早期通过人为标注词的手段去解决同义词、词性标注等问题。这同样存在一些问题，比如：1)不能分辨细节的差别；2)需要大量的人力劳动；3)存在很多主观因素；4)无法发现新词；5)难以精确计算词之间的相似度

## 离散表示
举个栗子
```
红色的大型卡车  [1,0,0,0,0,...,0]
白色的中型轿车  [0,1,0,0,0,...,0]
蓝色的小型电动车    [0,0,1,0,0,...,0]
```
像上面这个例子如果用one-hot表示的话，每一句话需要单独的一个向量来表示，这种方法会造成维度灾难和词汇鸿沟的问题，那么使用分散式的表示方法可以将上面三句话通过颜色、型号和车型来表示
```
一句话 = 颜色 * 型号 * 车型
```
这样就只需将颜色、型号和车型表示成词向量，而不必每一句类似的话都要通过单独的词向量来表示。

Distributed Representation表示一句话可以理解，那如何表示一个词呢？这里就可以引出现代统计自然语言处理中最有创新的想法之一：用一个词附近的其他词可以表示该词。

但是这带来了一些问题，该词周围的多少个词才能描述它？随着预料的增大，可以描述该词的词很多，一样会出现维度过高的问题，那么最直接的想法就是通过SVD对维度过高的向量矩阵进行降维处理同时保留其中的信息，但是通过SVD处理后得到的向量不怎么适合作为其他神经网络模型的输入，所以才出现Word2Vec这种方法。Word2Vec使用简单的神经网络模型通过这个词周围的词来表示该词，词表示方式是一个稠密的Distributed Representation（稠密的分散式向量）

## Word2Vec的前身NNLM模型
NNLM（Neural Network Language Model）在2003年由徐伟提出。如下图是NNLM模型的结构
![NNLM](/images/w2v/NNLM.jpg)
这个结构比较简单，从上到下，分别是输入层、映射层、隐藏层和输出层，很常见的神经网络的结构，那么为了更清晰的理解NNLM的过程，先要明白NNLM要做的事情和目标
假设现在有一个语料库，其中有10w个词，那么：
NNLM要做的事情：定义一个滑动窗口，比如这个滑动窗口大小是3，滑动窗口会遍历整个语料库，每次输入3个单词，然后预测下个词
结合上面NNLM的模型图，输入的三个词分别就是：
$$w{t-n+1},w{t-2},w_{t-1}$$
预测的词就是：
$$w_t=i|context$$

NNLM的目标：将预料库中的10w个词通过稠密的向量矩阵来表示，这个稠密向量矩阵在NNLM模型中就是：
$$Matrix C$$
如果你了解神经网络，就会发现NNLM有趣的地方，一般的神经网络训练数据时通过反向传播算法优化神经网络中的参数，从而让输出层输出我们想要的结果，而NNLM不同，我们最终想要的记过是神经网络训练时通过反向传播算法优化的这些参数，这些参数就是语料库中词的词向量

明白了NNLM要做的事情和目标，接下来就详细解释下NNLM的整个流程
1. 滑动窗口获得one-hot形式的词向量输入，NNLM模型中获得3个one-hot形式的词向量
$$w{t-n+1}, w{t-2}, w_{t-1}$$
2. 一开始，随机初始化300*10w的矩阵Matric C，这就是我们最终需要的词向量，工业界认为，在大的预料库上300维~500维才能表示出词的含义和关系
$$C = (W_1, W2,..., W(10w))$$
3. Matric C与one-hot做映射，就是300*10w矩阵与1*10w的矩阵的转置矩阵相乘，结果是300*1的矩阵，相当于将one-hot形式词向量中1的位置对应到300*10w矩阵的响应位置给切割下来，这也就是映射层的作用，通过映射，将3个one-hot表示的词向量转换为3个300*1的向量NNLM模型图中就是
$$C{(w{t-n})},C{(w{t-2})},C{(w{t-1})}$$
4. 将映射获得的向量做一个简单链接，这里获得3个300*1的向量，通过链接就可以获得900*1的向量
5. 假设NNLM的隐藏层有500维，那么这里要做的就是讲刚刚链接好的900维向量与500维的向量做一个全连接，然后隐藏层再与输出层做一个全连接，从NNLM模型图中可以看出隐藏层与输出层直接使用了tanh双曲正切作为激活函数
6. 输出层是10w维，它跟语料库是一样大，最后使用softmax函数对输出岑的内容进行处理，将输出层10w维的结果向量变成一个概率分布，那么10w维向量中，概率最大的位置就是要预测的词
7. 获得预测结果后与真实结果进行比对，计算预测结果与真实结果的cross entropy loss交叉损失，通过反向传播算法优化该神经网络的参数达到最小化交叉熵损失，从而获得稠密词向量矩阵

这就是NNLM模型的整个流程。

## Word2Vec
严格来讲NNLM的效果还是不错的，但是计算量太大，通过NNLM训练一个语料库耗时太久，为了降低计算量的同时保证效果，Tomas Mikolov提出了word2vec，在word2vec中有两种模型，分别是CBOW(Continuous Bag-of-Words Model)连续词袋模型和Skip-Gram模型(Continuous Skip-Gram Model)
![word2vec模型](/images/w2v/word2vec模型.jpg)
从图中可以看出两个模型是相反的，CBOW模型是通过周围的词来预测中间的词，而Skip-Gram模型正好相反，它是通过中间的词来预测周围的词。
与NNLM模型相比，CBOW的模型显得更加简单，它其实就是将NNLM中不必要的计算给去除了，CBOW流程也比较简单
1. 使用双向的滑动窗口获得词的稠密向量而不是one-hot表示的词向量，这个稠密向量一开始由我们随机初始化，在训练过程中优化它
2. 去掉NNLM中的投影层和隐藏层（因为输入的就是稠密向量，所以投影层对CNOW没有意义），对输入层输入的稠密向量进行简单的求和或者求平均，图中CBOW模型是SUM求和
3. 然后再与输出层进行全连接，通过层次的softmax将输出层的数据转换为概率分布，与真实的概率进行对比，优化CBOW模型的输入，最小化两个概率分析的差距
训练10w大小的预料库，输出层就需要10w维，这个维度有点高使得计算量很大，而层次softmax就是解决这个问题的方法之一，如下图
![CBOW](/images/w2v/CBOW.jpg)
使用Huffman编码来编码输出层向量，这样在计算某个词时就需要计算路径上所有非叶子节点词向量的权重则可，将计算量降为树的深度
除了层次softmax这个方法，还可以通过负例采样来解决这个问题
以为输出层的维度太高计算复杂，而最终的结果其实只有一个是正确的答案，也就是我们要的词，而其他都是错误的答案，如果语料库是10w，那么输出层就有10w维，在这10w维中，只有一个是正确答案，10w-1都是错误答案，在训练时我们不需要这么多错误答案，那么从中抽取一些错误答案组成一个低纬度的向量用来训练模型就好了，比如只要一个500维的向量进行训练，那么就需要从10w-1错误答案中抽取出499个错误答案，这些错误答案就是负例。
随机抽取肯定不靠谱，因为在预料库中，每个词出现的频率不同，所以要保证频次越高的词应该越容易被采样到，负例采样也是这个思想，我们将长度为1的线段平均分为M份，不同词频的词所占的比例不同，然后随机选择，选到哪个词这个词就是负例，如果选中了正例就重新投。
![负例采样](/images/w2v/负例采样.jpg)
之所以要将线段分为M份，是为了速度，这样每次就不用生成0-1之间的随机数，而生成0-M之间整数，然后直接去这个刻度上抽一个单词即可。
在数据量比较小的情况下使用CBOW效果比较好，而在数据量巨大的时候，使用Skip-Gram效果比较好

## word2vec的不足
word2vec算是NLP中的非常重要的东西，无论你使用什么神经网络，如果你要对语言进行训练，第一步就是对语言进行翻译，将其表示成一个词向量，而word2vec就是其中比较好的实现，但它也有一些不足
1. 没有利用预料库全局的信息，从刚刚的模型中也可以看出，word2vec对每个滑动窗口中的词向量进行训练，无法利用全局信息，这点可以使用Glove，一个利用全局信息的模型
2. 对多义词无解，因为使用了唯一的词向量，比如Apple，表示的是吃的苹果还是苹果手机